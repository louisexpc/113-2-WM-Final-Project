{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Session Generator Without Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/guest/r13946024/.local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import ast\n",
    "from utils import load_pickle\n",
    "from sentence_transformers import SentenceTransformer, util"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part I Load model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 2/2 [00:07<00:00,  3.61s/it]\n",
      "Some parameters are on the meta device because they were offloaded to the cpu.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n",
    "\n",
    "model_name = \"meta-llama/Llama-2-7b-chat-hf\"\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(model_name, device_map=\"auto\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_product_name(raw_output, fallback=\"Unknown Product\"):\n",
    "    # Split the response into lines\n",
    "    lines = raw_output.strip().split(\"\\n\")\n",
    "\n",
    "    # Find the last non-empty line\n",
    "    for line in reversed(lines):\n",
    "        clean_line = line.strip()\n",
    "        if clean_line:\n",
    "            # Remove common prefixes\n",
    "            for prefix in [\"Sure,\", \"Here is\", \"Here are\", \"Of course,\", \"I'd be happy to help!\"]:\n",
    "                if clean_line.lower().startswith(prefix.lower()):\n",
    "                    clean_line = clean_line[len(prefix):].strip()\n",
    "            # Remove category hints if present\n",
    "            clean_line = re.sub(r'for the \".+?\" category', \"\", clean_line).strip()\n",
    "            \n",
    "            # Return the cleaned line if it looks like a product name\n",
    "            if clean_line and len(clean_line.split()) >= 2:\n",
    "                return clean_line\n",
    "\n",
    "    # Fallback if no valid name is found\n",
    "    print(f\"⚠️ No valid product name found in: '{raw_output}'\")\n",
    "    return fallback\n",
    "\n",
    "# === Extract a Python List from LLM Output ===\n",
    "def extract_list_from_text(text, fallback=None):\n",
    "    if not text or not isinstance(text, str):\n",
    "        print(\"⚠️ Invalid input. Using fallback.\")\n",
    "        return fallback or []\n",
    "\n",
    "    # Strip leading instructions and whitespace\n",
    "    cleaned_text = text.strip()\n",
    "\n",
    "    # Extract the first valid list\n",
    "    match = re.search(r'\\[\\s*(?:[^\\[\\]]|\\[[^\\[\\]]*\\])*\\s*\\]', cleaned_text, re.DOTALL)\n",
    "    if match:\n",
    "        try:\n",
    "            extracted = match.group(0).replace(\"\\n\", \"\").replace(\"    \", \"\").strip()\n",
    "            # Basic sanity check to avoid explanation leakage\n",
    "            if extracted.startswith(\"[\") and extracted.endswith(\"]\"):\n",
    "                return ast.literal_eval(extracted)\n",
    "            else:\n",
    "                print(\"⚠️ Unexpected non-list format. Using fallback.\")\n",
    "        except Exception as e:\n",
    "            print(f\"⚠️ Parsing failed: {e}\")\n",
    "    \n",
    "    print(\"⚠️ No valid list found. Using fallback.\")\n",
    "    return fallback or []"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part II Functions & Prompts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## prompt #1: Get Categories form History Sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_categories_from_history(history_str, category_list, n=9):\n",
    "    categories_str = \", \".join(category_list)\n",
    "    # Define the system and user prompts\n",
    "    system_prompt = \"You are a fashion recommendation assistant.\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Given this purchase history:\n",
    "    {history_str}\n",
    "\n",
    "    Select exactly {n} categories from the following list:\n",
    "    {categories_str}\n",
    "\n",
    "    ⚠️ Respond **only** with a Python list of exactly {n} categories, **without any additional text, punctuation, or explanations.**\n",
    "\n",
    "    ✅ Format exactly like this:\n",
    "    [\"Category1\", \"Category2\", \"Category3\", ..., \"Category{n}\"]\n",
    "\n",
    "    ❗ Do not add any introductory words, explanations, or context.\n",
    "    ❗ Do not include line breaks within the list.\n",
    "    ❗ Do not add extra punctuation or bullet points.\n",
    "    \"\"\"\n",
    "\n",
    "    prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt} [/INST]\"\n",
    "    \n",
    "    inputs = tokenizer(prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    response = response.split(\"[/INST]\")[-1].strip()\n",
    "    return response\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prompt #2: Generate New Session from Categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_item_from_category(category):\n",
    "    # Define the system and user prompts\n",
    "    system_prompt = \"You are a fashion product naming assistant.\"\n",
    "    user_prompt = f\"\"\"\n",
    "    Generate a single, catchy, and realistic product name for the \"{category}\" category.\n",
    "\n",
    "    ⚠️ **Output Requirements:**  \n",
    "    - Respond with only the product name as a single line of text.  \n",
    "    - Do not include quotes, bullet points, or explanations.  \n",
    "    - Use realistic, catchy names like \"Spanx Alot Swimsuit\" or \"Jerry Jogger Bottoms\".\n",
    "\n",
    "    ❗ **Examples:**  \n",
    "    Slim-fit T-shirt  \n",
    "    Leather Jacket  \n",
    "    Spanx Alot Swimsuit  \n",
    "    \"\"\"\n",
    "\n",
    "    # Construct the full prompt\n",
    "    full_prompt = f\"<s>[INST] <<SYS>>\\n{system_prompt}\\n<</SYS>>\\n\\n{user_prompt.strip()} [/INST]\"\n",
    "\n",
    "    # Generate the response\n",
    "    inputs = tokenizer(full_prompt, return_tensors=\"pt\").to(\"cuda\")\n",
    "    outputs = model.generate(**inputs, max_new_tokens=128)\n",
    "\n",
    "    # Decode and return the response\n",
    "    response = tokenizer.decode(outputs[0], skip_special_tokens=True).strip()\n",
    "    response = response.split(\"[/INST]\")[-1].strip()\n",
    "    return response\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Utils Function\n",
    "\n",
    "- **prefilter_categories(items_list, category_list, top_k=20)**\n",
    "\n",
    "    此函式用於在進入 LLM 前，根據使用者歷史購買商品所對應的 product type（商品類別），從所有可選類別中預先篩選出最相關的前 top_k 個類別，避免因為類別數量過多導致超出 LLM 的上下文長度限制（context window overflow）。\n",
    "\n",
    "- **article_to_product_type(article_id)**\n",
    "\n",
    "    此函式將 articleId 轉換成相對應的 product type (category)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Pre-Filter Categories with Embeddings ===\n",
    "def prefilter_categories(items_list, category_list, top_k=20):\n",
    "    model = SentenceTransformer(\"all-MiniLM-L6-v2\")\n",
    "    \n",
    "    # Encode the history as a single vector\n",
    "    history_embedding = model.encode(items_list, convert_to_tensor=True)\n",
    "    \n",
    "    # Encode all categories\n",
    "    category_embeddings = model.encode(category_list, convert_to_tensor=True)\n",
    "    \n",
    "    # Compute similarity\n",
    "    cos_scores = util.cos_sim(history_embedding, category_embeddings)[0]\n",
    "    top_results = cos_scores.topk(k=top_k)\n",
    "    \n",
    "    # Extract top-K categories\n",
    "    top_categories = [category_list[i] for i in top_results.indices]\n",
    "    top_scores = [cos_scores[i].item() for i in top_results.indices]\n",
    "    \n",
    "    # Return as a dictionary for better interpretability\n",
    "    return top_categories\n",
    "\n",
    "\n",
    "def article_to_product_type(article_id):\n",
    "    mapping = load_pickle(\"data/article_to_product_mapping.pkl\")\n",
    "    return mapping.get(article_id, \"Unknown Product Type\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Pipeline\n",
    "\n",
    "**enrich_user_session(user_sessions, category_list, n=9, max_history_length=10)**\n",
    "\n",
    "這個函式是整個推薦系統流程的核心，它的目的在於將原本的使用者購買紀錄 user session 擴充為更完整、更擬真的購物序列，藉由語意相似度 + LLM 輔助推理生成更多合理的商品。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# === Generate a New Enriched User Session ===\n",
    "def enrich_user_session(user_sessions, category_list, n=9, max_history_length=10):\n",
    "    session = {uid: items.copy() for uid, items in user_sessions.items()}\n",
    "\n",
    "    for uid, items in session.items():\n",
    "        # Convert item IDs to product names\n",
    "        history_str = \" \".join([article_to_product_type(item) for item in items])\n",
    "        print(f\"User: {uid}, History: {history_str}\\n\")\n",
    "        top_categories = prefilter_categories(history_str, category_list, top_k=20)\n",
    "        print(f\"Top categories: {top_categories}\\n\")\n",
    "        # Generate Categories\n",
    "        raw_output = get_categories_from_history(history_str, top_categories)\n",
    "        print(\"LLM raw output:\", raw_output, \"\\n\")\n",
    "        print(\"End of the output\")\n",
    "        categories = extract_list_from_text(raw_output, fallback=[\"tops\", \"accessories\", \"shoes\"])\n",
    "        print(f\"Selected Categories: {categories}\\n\")\n",
    "\n",
    "        # Generate Items for Each Category\n",
    "        while len(items) < max_history_length:\n",
    "            for category in categories:\n",
    "                raw_item = get_item_from_category(category)\n",
    "                print(\"Raw item LLM output: \\n\", raw_item)\n",
    "                item = extract_product_name(raw_item, fallback=\"Unkown\")\n",
    "                print(\"Extracted: \", item)\n",
    "                if len(items) > 0:\n",
    "                    items.insert(-1, item)\n",
    "                else:\n",
    "                    # If the list is empty, just append\n",
    "                    items.append(item)\n",
    "                if len(items) >= max_history_length:\n",
    "                    break\n",
    "\n",
    "    return session"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part III Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load pickle file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Load Category List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total categories loaded: 131\n",
      "Sample categories: ['Vest top', 'Bra', 'Underwear Tights', 'Socks', 'Leggings/Tights', 'Sweater', 'Top', 'Trousers', 'Hair clip', 'Umbrella']\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "categories_df = pd.read_csv(\"product_types.csv\")\n",
    "category_list = categories_df[\"category_name\"].tolist()\n",
    "print(\"Total categories loaded:\", len(category_list))\n",
    "print(\"Sample categories:\", category_list[:10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Test Category Prefiltering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test Session Enrichment\n",
    "\n",
    "This block is only for demonstration, not real data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User: 00000dbacae5abe5e23885899a1fa44253a17956c6d1c3d25f88aa139fdfc657, History: T-shirt Top Jacket Vest top Sweater Blazer\n",
      "\n",
      "Top categories: ['Vest top', 'T-shirt', 'Sweater', 'Blazer', 'Jacket', 'Tailored Waistcoat', 'Swimwear top', 'Hoodie', 'Shirt', 'Garment Set', 'Outdoor Waistcoat', 'Polo shirt', 'Bodysuit', 'Bikini top', 'Swimsuit', 'Swimwear bottom', 'Swimwear set', 'Clothing mist', 'Zipper head', 'Robe']\n",
      "\n",
      "LLM raw output: Sure, here are the 9 categories based on the purchase history:\n",
      "\n",
      "[\"Vest top\", \"T-shirt\", \"Sweater\", \"Blazer\", \"Jacket\", \"Tailored Waistcoat\", \"Swimwear top\", \"Hoodie\", \"Shirt\"] \n",
      "\n",
      "End of the output\n",
      "Selected Categories: ['Vest top', 'T-shirt', 'Sweater', 'Blazer', 'Jacket', 'Tailored Waistcoat', 'Swimwear top', 'Hoodie', 'Shirt']\n",
      "\n",
      "Raw item LLM output: \n",
      " Sure, here's a product name for the \"Vest top\" category:\n",
      "\n",
      "FlexFit Vest\n",
      "Extracted:  FlexFit Vest\n",
      "Raw item LLM output: \n",
      " FreshFit Tee\n",
      "Extracted:  FreshFit Tee\n",
      "Raw item LLM output: \n",
      " Sure, here's a product name for the \"Sweater\" category:\n",
      "\n",
      "FuzzyFold Fluff Top\n",
      "Extracted:  FuzzyFold Fluff Top\n",
      "User: 0000423b00ade91418cceaf3b26c6af3dd342b51fd051eec9c12fb36984420fa, History: Dress Dress Cardigan Bikini top Swimsuit Vest top\n",
      "\n",
      "Top categories: ['Swimwear top', 'Bikini top', 'Vest top', 'Swimsuit', 'Swimwear bottom', 'Swimwear set', 'Bodysuit', 'Dress', 'Tailored Waistcoat', 'Night gown', 'Sweater', 'Garment Set', 'Cardigan', 'Costumes', 'Jacket', 'T-shirt', 'Outdoor Waistcoat', 'Underwear body', 'Robe', 'Hoodie']\n",
      "\n",
      "LLM raw output: Sure, here are the 9 categories based on the given purchase history:\n",
      "\n",
      "[\"Swimwear top\", \"Bikini top\", \"Vest top\", \"Swimsuit\", \"Swimwear bottom\", \"Swimwear set\", \"Dress\", \"Cardigan\", \"Night gown\"] \n",
      "\n",
      "End of the output\n",
      "Selected Categories: ['Swimwear top', 'Bikini top', 'Vest top', 'Swimsuit', 'Swimwear bottom', 'Swimwear set', 'Dress', 'Cardigan', 'Night gown']\n",
      "\n",
      "Raw item LLM output: \n",
      " Sure, here's a product name for the \"Swimwear top\" category:\n",
      "\n",
      "Riptide Rush Top\n",
      "Extracted:  Riptide Rush Top\n",
      "Raw item LLM output: \n",
      " Sure, I'd be happy to help! Here is a single, catchy, and realistic product name for the \"Bikini top\" category:\n",
      "\n",
      "FlauntFlex Bikini Top\n",
      "Extracted:  FlauntFlex Bikini Top\n",
      "Raw item LLM output: \n",
      " Sure, here's a product name for the \"Vest top\" category:\n",
      "\n",
      "FlexFit Vest\n",
      "Extracted:  FlexFit Vest\n",
      "Enriched Session: {'00000dbacae5abe5e23885899a1fa44253a17956c6d1c3d25f88aa139fdfc657': [841260003, 887593002, 890498002, 795440001, 859416011, 'FlexFit Vest', 'FreshFit Tee', 'FuzzyFold Fluff Top', 568601043], '0000423b00ade91418cceaf3b26c6af3dd342b51fd051eec9c12fb36984420fa': [759191008, 800436010, 814686001, 590928022, 698276006, 'Riptide Rush Top', 'FlauntFlex Bikini Top', 'FlexFit Vest', 749699013]}\n",
      "執行時間：157.2823 秒\n"
     ]
    }
   ],
   "source": [
    "import time \n",
    "\n",
    "start_time = time.time()\n",
    "user_session = {\n",
    "    \"00000dbacae5abe5e23885899a1fa44253a17956c6d1c3d25f88aa139fdfc657\": [841260003, 887593002, 890498002, 795440001, 859416011, 568601043],\n",
    "    \"0000423b00ade91418cceaf3b26c6af3dd342b51fd051eec9c12fb36984420fa\": [759191008, 800436010, 814686001, 590928022, 698276006, 749699013],\n",
    "}\n",
    "\n",
    "new_session = enrich_user_session(user_session, category_list, n=3, max_history_length=9)\n",
    "print(\"Enriched Session:\", new_session)\n",
    "end_time = time.time()\n",
    "elapsed_time = end_time - start_time\n",
    "print(f\"執行時間：{elapsed_time:.4f} 秒\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "web-mining",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
