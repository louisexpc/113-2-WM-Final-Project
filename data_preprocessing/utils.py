import pandas as pd
import numpy as np
import os.path as path
from typing import Dict, Any, List, Tuple, Optional
import pickle
from itertools import chain
from pathlib import Path
from random import Random
from tqdm import tqdm

def filter_cold_start_articles(df: pd.DataFrame, min_purchases: int = 5) -> pd.DataFrame:
    """
    Remove cold-start articles based on minimum interaction threshold.

    Args:
    - df: DataFrame, must contain column 'article_id'. 
          Typically includes [t_dat, customer_id, article_id, price, sales_channel_id]
    - min_purchases : int, default 5. The minimum number of total purchases an article must have to be retained in the dataset.

    Return:
    - filtered_df : DataFrame containing only the transactions where article_id has appeared at least `min_purchases` times. Index is reset.
    """
    df = df.copy()
    article_counts = df.groupby('article_id')['article_id'].transform('count')

    return df[article_counts >= min_purchases].reset_index(drop=True)


def filter_weeks_length(df, weeks=24, min_purchases=4):
    """
    Filter user transactions based on their last active date and minimum purchase count.

    Args:
    - df             : DataFrame, must contain cols: [t_dat, customer_id, article_id, price, sales_channel_id]
    - weeks          : int, default 24. The number of weeks to retain from each user's last transaction date.
    - min_purchases  : int, default 4. Minimum number of transactions required for a user to be retained.

    Return:
    - filtered_df : DataFrame containing only transactions that occurred within the [last_date - weeks, last_date]
                   time window for each customer, and only for customers with at least `min_purchases` transactions.
                   The returned DataFrame is sorted as in original and reset index.
    """

    df = df.copy()
    df['t_dat'] = pd.to_datetime(df['t_dat'])

    df['customer_id'] = df['customer_id'].astype('category')

    last_date  = df.groupby('customer_id')['t_dat'].transform('max')
    row_count  = df.groupby('customer_id')['t_dat'].transform('size')

    start_date = last_date - np.timedelta64(weeks*7, 'D')

    mask = (df['t_dat'] >= start_date) & (df['t_dat'] <= last_date) & (row_count >= min_purchases)

    return df[mask].reset_index(drop=True)

def transaction_to_session(df: pd.DataFrame, cast: bool = True, save_path = None, pkl_name = str) -> Dict[str, Dict[str, list]]:
    """
    Turn transaction record into seesion based data 

    Args:
    - df   : DataFrame, must contain cols: [t_dat, customer_id, article_id, price, sales_channel_id]
    - cast : bool, default True. Force to turn into correct dtype for each cols

    Return:
    - sessions : {
        customer_id: {
            'article_id'      : [int32, ...],
            't_dat'           : [Timestamp, ...],
            'price'           : [float32, ...],
            'sales_channel_id': [uint8, ...]
        },
        ...
     }
    """
    if cast:
        df = df.astype({
            'article_id'      : 'int32',
            'price'           : 'float32',
            'sales_channel_id': 'uint8'
        }).copy()
        df['t_dat'] = pd.to_datetime(df['t_dat'])

    session_df = (
        df.groupby('customer_id', sort=False)
          .agg({
              'article_id'      : list,
              't_dat'           : list,
              'price'           : list,
              'sales_channel_id': list
          })
    )

    sessions = session_df.to_dict(orient='index')

    if save_path is not None:
        if pkl_name is None:
            raise ValueError("pkl_name must be provided if save_path is set.")
        full_path = path.join(save_path,f"{pkl_name}.pkl")
        with open(full_path, "wb") as f:
            pickle.dump(sessions, f, protocol=pickle.HIGHEST_PROTOCOL)
    return sessions



def baseline_transformation(
    sessions: Dict[str, Dict[str, List[int]]],
    num_neg: int = 99,
    seed: int = 42,
    user_session_path: Optional[str] = None,
    testing_path: Optional[str] = None,
) -> Tuple[Dict[str, List[int]], Dict[str, List[int]]]:
    """
    Turn session dict into : 
    1. user_session  : {uid: [iid1, iid2, ...]}          For training/validation
    2. testing_data  : {uid: [neg1..neg99, test_item]}   For leave-one-out test

    Args:
    ----
    - sessions          : dict generated by func transaction_to_session 
    - num_neg           : int, num of negative sampling
    - seed              : random seed
    - user_session_path : path for saving  user_session pickle, default None
    - testing_path      : path for saving testing_data pickle, default None
    
    Return
   
    -   user_session : Dict[str, List[int]]
            { uid: [iid_1, iid_2, ..., iid_n] }
    -   testing_data : Dict[str, List[int]]
            { uid: [neg_1, ..., neg_99, test_item] }
    """

    rng = Random(seed)

    user_session: Dict[str, List[int]] = {}
    all_items_set: set[int] = set()

    for uid, rec in sessions.items():
        items = rec["article_id"]
        user_session[uid] = items
        all_items_set.update(items)

    all_items_arr = np.fromiter(all_items_set, dtype=np.int32)

    testing_data: Dict[str, List[int]] = {}

    for uid, items in tqdm(user_session.items(), desc="Creating Data...",unit= " item"):
        pos_item = items[-1]       
        positives = set(items)

        negs: List[int] = []
        while len(negs) < num_neg:
            cand = rng.sample(list(all_items_arr), k=num_neg * 3)
            negs.extend(x for x in cand if x not in positives)
            negs = negs[:num_neg]

        testing_data[uid] = negs + [pos_item]
    def _dump(obj: dict, out_path: Optional[str]):
        if out_path:
            Path(out_path).parent.mkdir(parents=True, exist_ok=True)
            with open(out_path, "wb") as f:
                pickle.dump(obj, f, protocol=pickle.HIGHEST_PROTOCOL)

    _dump(user_session, user_session_path)
    _dump(testing_data, testing_path)

    return user_session, testing_data

if __name__ == '__main__':
    pass